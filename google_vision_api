#!/usr/bin/env python
from IPython.display import display
import io
import os
from google.cloud import vision
from google.cloud.vision_v1 import types
import pandas as pd
from PIL import Image
import tempfile
from PIL import Image
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase import pdfmetrics
from io import BytesIO
#ignore warning, can be remove
import warnings
warnings.filterwarnings("ignore")
from PIL import Image, ImageFilter, ImageEnhance
import requests
import json
import urllib.request
import csv
import yaml
import PyPDF2


config_file_path = 'config.yml'
with open(config_file_path, 'r') as file:
    config = yaml.safe_load(file)

# Accessing configuration data
csv_input_file = config['csv_input_file']
csv_output_file = config['csv_split_output_file']
csv_combine_output_file = config['csv_combine_output_file']
font_path = config['font_path']
image_directory_path = config['image_directory_path']
GOOGLE_APPLICATION_CREDENTIALS = config['GOOGLE_APPLICATION_CREDENTIALS']
# please see the google_API_intro.pdf to see how to get api key json file
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = GOOGLE_APPLICATION_CREDENTIALS

# read csv from input node and return link
def read_csv_from_node(csv_input_file):
    data = pd.read_csv(csv_input_file)
    return(data)
# return Example
'''
   Node_id                 Title          Language
0    16264             Sushi One  English,Japanese
1    16267  Harden & L. C. Corp.   English,Chinese
2    16270              Sung Lin   English,Chinese
'''

# process data from node
def process_node(node_id):
    res = {}
    res[node_id] = {}
    with urllib.request.urlopen("https://menus.digital.utsc.utoronto.ca/node/" + str(node_id) + "/children_rest") as url:
        node_data_from_link = json.load(url)
        # if node id is incorrect (unable to access) skip
        # from the web end the page will return []
        if len(node_data_from_link) > 1:
            for node_data in node_data_from_link:
                res[node_id][node_data['nid']] = {'Media_id':node_data['field_islandora_object_media'] , 'Title':node_data['title']}
            return res
        return 0

'''
{16264: 
{'16266': {'Media_id': '36086', 'Title': 'Sushi One - Page 1'}, 
'16265': {'Media_id': '36085', 'Title': 'Sushi One - Page 2'}}}
'''

# access link after read_csv_from_node 
import urllib.request, json 
def access_node(data):
    # access from node
    res = {}
    for node_id in data['Node_id']:
        # if def process_node return is not equal 0 update date to res 
        if process_node(node_id) != 0:
            res.update(process_node(node_id))
    return res

# example return
'''
{'16266': {'Media_id': '36086', 'Title': 'Sushi One - Page 1'}, 
'16265': {'Media_id': '36085', 'Title': 'Sushi One - Page 2'}, 
'16268': {'Media_id': '36087', 'Title': 'Harden &amp; L. C. Corp. - Page 1'}, 
'16269': {'Media_id': '36088', 'Title': 'Harden &amp; L. C. Corp. - Page 2'}, 
'16272': {'Media_id': '36090', 'Title': 'Sung Lin - Page 1'}, 
'16271': {'Media_id': '36089', 'Title': 'Sung Lin - Page 2'}}
'''

# process media after we have the result for access_node
def process_media(nid_title_media_id):
    for nodeid in list(nid_title_media_id.keys()):
        for node in list(nid_title_media_id[nodeid].keys()):
            with urllib.request.urlopen("https://menus.digital.utsc.utoronto.ca/media/" + str((nid_title_media_id[nodeid][node]['Media_id'])) + "?_format=json") as url:
                media_data_from_link = json.load(url)
                target_id = media_data_from_link.get('field_media_image')[0]['target_id']
                # if target_id is not empty add target_id else remove node id
                if (target_id) > 0:
                    nid_title_media_id[nodeid][node]['target_id'] = target_id
                else:
                    del nid_title_media_id[nodeid][node]
    return(nid_title_media_id)
# return example
'''
{'16266': {'Media_id': '36086', 'Title': 'Sushi One - Page 1', 'target_id': 59809}, 
'16265': {'Media_id': '36085', 'Title': 'Sushi One - Page 2', 'target_id': 59806}, 
'16268': {'Media_id': '36087', 'Title': 'Harden &amp; L. C. Corp. - Page 1', 'target_id': 59812}, 
'16269': {'Media_id': '36088', 'Title': 'Harden &amp; L. C. Corp. - Page 2', 'target_id': 59815}, 
'16272': {'Media_id': '36090', 'Title': 'Sung Lin - Page 1', 'target_id': 59821},
'16271': {'Media_id': '36089', 'Title': 'Sung Lin - Page 2', 'target_id': 59818}}
'''

def process_file(mid_data):
    base_dir = os.path.expanduser(image_directory_path+'original_image')
    if not os.path.exists(base_dir):
        os.makedirs(base_dir)
    for nodeid in list(mid_data.keys()):
        for node in list(mid_data[nodeid].keys()):
            with urllib.request.urlopen("https://menus.digital.utsc.utoronto.ca/file/" + str((mid_data[nodeid][node]['target_id'])) + "?_format=json") as url:
                media_data_from_link = json.load(url)
                url_link = media_data_from_link['uri'][0]['url']
                local_file_name = url_link.split('/')[-1] 
                title_name = mid_data[nodeid][node]['Title'].replace(" ", "")
                title_name = title_name.replace("-", "")
                local_file_name = str(nodeid)+ '_' + str(node) + '_' + title_name + '_' + local_file_name
                full_path = os.path.join(base_dir,local_file_name)
                mid_data[nodeid][node]['local_path'] = full_path
                response = requests.get("https://menus.digital.utsc.utoronto.ca" + url_link)
                if response.status_code == 200:
                    with open(full_path, 'wb') as file:
                        file.write(response.content)

# save file to local path and full out format csv. Input csv
def get_mid_data(csv_input_file):
    node_data = read_csv_from_node(csv_input_file)
    nid_title_media_id = access_node(node_data)
    mid_data =  process_media(nid_title_media_id)
    return mid_data
def download_file(csv_input_file):
    mid_data = get_mid_data(csv_input_file)
    process_file(mid_data)

# read all the images from directory or local path
def find_all_img_path(image_directory_path):
    abs_path = os.path.join(image_directory_path, 'original_image')
    image_extensions = ['.jpg', '.png', '.jp2', '.tiff']
    image_paths = []
    desired_prefix = 'OBJ.0_'
    # Walk through the directory
    for root, dirs, files in os.walk(abs_path):
        for file in files:
            full_path = os.path.join(root, file)
            if any(file.endswith(ext) for ext in image_extensions) and desired_prefix in file:
                image_paths.append(full_path)

    return image_paths
# return example
'''
"\n\n['C:\\Users\\zxx91\\OBJ.0_10762.jp2',
\n'C:\\Users\\zxx91\\OBJ.0_10763.jp2', 
\n'C:\\Users\\zxx91\\OBJ.0_10764.jp2', 
\n'C:\\Users\\zxx91\\OBJ.0_10765.jp2', 
\n'C:\\Users\\zxx91\\OBJ.0_10766.jp2', 
\n'C:\\Users\\zxx91\\OBJ.0_10767.jp2']\n"
'''

# Initialize the client for the Vision API
client = vision.ImageAnnotatorClient()
def detectText(img):
    # Check if the file is a JP2 file
    with Image.open(img) as opened_image:
        # Convert to RGB if not already in JPEG format (necessary for non-JPEG formats)
        if opened_image.format != 'JPEG':
            with io.BytesIO() as output:
                opened_image.convert('RGB').save(output, format="JPEG")
                content = output.getvalue()
        else:
            # For JPEG (including JP2 handled by Pillow), read directly
            with io.open(img, 'rb') as image_file:
                content = image_file.read()
    # Prepare the image for the Vision API
    image = types.Image(content=content)

    # Perform text detection
    response = client.text_detection(image=image)
    texts = response.text_annotations
    return texts

# read all images path from local path and save to target path
def save_json(image_directory_path):
    base_dir = os.path.expanduser(image_directory_path+'json')
    if not os.path.exists(base_dir):
        os.makedirs(base_dir)
    all_img_path = find_all_img_path(image_directory_path)
    json_file_path = []
    for img in all_img_path:
        image_path = img.split('\\')[-1]
        node_id = image_path.split('_')[0]
        media_id = image_path.split('_')[1]
        node_dir = os.path.expanduser(base_dir+'\\'+str(node_id))
        media_dir = os.path.expanduser(node_dir+'\\'+str(media_id))
        if not os.path.exists(media_dir):
            os.makedirs(media_dir)
        response = detectText(img)
        json_file_name = image_path.replace('.jp2', '.json')

        json_file_path = json_file_path + [media_dir+'\\'+json_file_name]
        # Extracting the data from the response
        extracted_data = [{
            'description': text.description,
            'boundingPoly': [{'x': vertex.x, 'y': vertex.y} for vertex in text.bounding_poly.vertices]
        } for text in response]
        # Writing the extracted data to a JSON file
        with open(media_dir+'\\'+json_file_name, 'w', encoding='utf-8') as file:
            json.dump(extracted_data, file, ensure_ascii=False, indent=4)
    return json_file_path

# generate pdf/A with selectable text and return file name
def create_pdf_with_selectable_text(media_dir,image_directory_path, texts, font_path):
    pdfmetrics.registerFont(TTFont('NotoSans', font_path))
    # Open the image and get its dimensions
    with Image.open(image_directory_path) as img:
        img_width, img_height = img.size
        scale_w = letter[0] / img_width
        scale_h = letter[1] / img_height
        # Save the image to a temporary file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as temp_img_file:
            img.save(temp_img_file, format='JPEG')
            temp_img_path = temp_img_file.name
        # Create a PDF
        pdf_path = image_directory_path.replace('.jp2', '.pdf')
        pdf_path = media_dir+pdf_path.split('\\')[-1]
        c = canvas.Canvas(pdf_path, pagesize=letter)
        # Add the image to the PDF
        c.drawImage(temp_img_path, 0, 0, width=letter[0], height=letter[1])
        # Add invisible text objects to the PDF
        for text in texts[1:]:  # Skip the first element
            vertices = [(int(vertex.x * scale_w), int(vertex.y * scale_h)) for vertex in text.bounding_poly.vertices]
            x0, y0 = vertices[0]
            y0 = letter[1] - y0 - (text.bounding_poly.vertices[2].y - text.bounding_poly.vertices[0].y) * scale_h

            # Set font size, invisible fill color and write text
            c.setFont("NotoSans", 12)
            c.setFillColorRGB(0, 0, 0, alpha=0)
            c.drawString(x0, y0, text.description)
        # Finalize the PDF
        c.save()
        # Remove the temporary image file
        os.remove(temp_img_path)
    return pdf_path

# call create_pdf_with_selectable_text to process all images from target image path
def generate_pdf(image_directory_path):
    base_dir = os.path.expanduser(image_directory_path+'pdf')
    if not os.path.exists(base_dir):
        os.makedirs(base_dir)
    all_img_path = find_all_img_path(image_directory_path)
    pdf_file_path = []
    for img in all_img_path:
        image_path = img.split('\\')[-1]
        node_id = image_path.split('_')[0]
        media_id = image_path.split('_')[1]
        node_dir = os.path.expanduser(base_dir+'\\'+str(node_id))
        media_dir = os.path.expanduser(node_dir+'\\'+str(media_id)+'\\')
        if not os.path.exists(media_dir):
            os.makedirs(media_dir)
        response = detectText(img)
        pdf_file = create_pdf_with_selectable_text(media_dir, img, response, font_path)
        pdf_file_path = pdf_file_path + [pdf_file]
    return pdf_file_path

# generate both json and pdf
def both_json_pdf(image_directory_path):
    json_file_path = save_json(image_directory_path)
    pdf_A_file_path = generate_pdf(image_directory_path)
    return json_file_path, pdf_A_file_path

# write csv file
def write_csv(csv_output_file, transformed_data, headers):
    with open(csv_output_file, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=headers)
        writer.writeheader()
        for row in transformed_data:
            row.pop('local_path', None) 
            writer.writerow(row)

# output csv with json
def output_csv_json(csv_input_file, csv_output_file, image_directory_path):
    mid_data = get_mid_data(csv_input_file)
    all_img_path = find_all_img_path(image_directory_path)
    json_file_path = save_json(image_directory_path)
    transformed_data = []
    headers = ["Main_id", "Node_id", "Title", "Media_id", "Local_File_Path", "OCR_File_Path"]
    img_json_pairs = zip(all_img_path, json_file_path)
    # Correct iteration over mid_data
    for main_id, nodes in mid_data.items():
        for node_id, details in nodes.items():
            if img_json_pairs: 
                img_path, json_path = next(img_json_pairs)
            row = {
                "Main_id": main_id,
                "Node_id": node_id,
                "Title": details['Title'],
                "Media_id": details['Media_id'],
                "Local_File_Path": img_path,
                "OCR_File_Path": json_path 
            }
            # Remove target_id if exists
            row.pop('target_id', None)
            transformed_data.append(row)
    write_csv(csv_output_file, transformed_data, headers)

# output csv with pdf/A
def output_csv_pdf(csv_input_file, csv_output_file, image_directory_path):
    mid_data = get_mid_data(csv_input_file)
    all_img_path = find_all_img_path(image_directory_path)
    pdf_file_path = generate_pdf(image_directory_path)
    transformed_data = []
    headers = ["Main_id", "Node_id", "Title", "Media_id", "Local_File_Path", "PDF_File_Path"]
    img_pdf_pairs = zip(all_img_path, pdf_file_path)
    # Correct iteration over mid_data
    for main_id, nodes in mid_data.items():
        for node_id, details in nodes.items():
            if img_pdf_pairs: 
                img_path, pdf_path = next(img_pdf_pairs)
            row = {
                "Main_id": main_id,
                "Node_id": node_id,
                "Title": details['Title'],
                "Media_id": details['Media_id'],
                "Local_File_Path": img_path,
                "PDF_File_Path": pdf_path 
            }
            # Remove target_id if exists
            row.pop('target_id', None)
            transformed_data.append(row)
    write_csv(csv_output_file, transformed_data, headers)

# output csv with pdf/A and json
def output_csv_pdf_json(csv_input_file, csv_output_file, image_directory_path):
    mid_data = get_mid_data(csv_input_file)
    json_file_path = save_json(image_directory_path)
    all_img_path = find_all_img_path(image_directory_path)
    pdf_file_path = generate_pdf(image_directory_path)
    transformed_data = []
    headers = ["Main_id", "Node_id", "Title", "Media_id", "Local_File_Path", "PDF_File_Path", "OCR_File_Path"]
    img_pdf_pairs = zip(all_img_path, pdf_file_path,json_file_path)
    # Correct iteration over mid_data
    for main_id, nodes in mid_data.items():
        for node_id, details in nodes.items():
            if img_pdf_pairs: 
                img_path, pdf_path, json_path = next(img_pdf_pairs)
            row = {
                "Main_id": main_id,
                "Node_id": node_id,
                "Title": details['Title'],
                "Media_id": details['Media_id'],
                "Local_File_Path": img_path,
                "PDF_File_Path": pdf_path,
                "OCR_File_Path": json_path
            }
            # Remove target_id if exists
            row.pop('target_id', None)
            transformed_data.append(row)
    write_csv(csv_output_file, transformed_data, headers)

#read multiple PDF files from path
def read_all_pdf(image_directory_path):
    abs_path = os.path.join(image_directory_path, 'pdf')
    pdf_extensions = ['.pdf']
    pdf_paths = []
    # Walk through the directory
    for root, dirs, files in os.walk(abs_path):
        for file in files:
            full_path = os.path.join(root, file)
            # Correctly check if file ends with any of the extensions in pdf_extensions
            if any(file.endswith(ext) for ext in pdf_extensions):
                pdf_paths.append(full_path)
    return pdf_paths

#Combining multiple PDF files into a single PDF
def combine_pdf(filtered_paths, output_filename):
    pdf_writer = PyPDF2.PdfWriter()
    for pdf_file in filtered_paths:
        # Open the PDF file
        with open(pdf_file, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            # Add all its pages to the writer object
            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                pdf_writer.add_page(page)
    
    # Write out the combined PDF to a new file
    with open(output_filename, 'wb') as out_file:
        pdf_writer.write(out_file)

# save multiple PDF files into a single PDF
def save_provessed_pdf(image_directory_path):
    abs_path = os.path.join(image_directory_path, 'pdf')
    pdf_path = read_all_pdf(image_directory_path)
    node_data = read_csv_from_node(csv_input_file)
    nid_title_media_id = access_node(node_data)
    node_list = []
    path_list = []
    for nodeid in list(nid_title_media_id.keys()):
        filtered_paths = [path for path in pdf_path if f'\\{nodeid}\\' in path.replace('/', '\\')]
        combine_pdf(filtered_paths, abs_path+'\\'+str(nodeid)+'\\'+str(nodeid)+'.pdf')
        node_list = node_list + [nodeid]
        path_list = path_list + [abs_path+'\\'+str(nodeid)+'\\'+str(nodeid)+'.pdf']
    headers = ["Main_id", "PDF_File_Path"]
    transformed_data = []
    for main_id, pdf_path in zip(node_list, path_list):
        transformed_data.append({"Main_id": main_id, "PDF_File_Path": pdf_path})
    write_csv(csv_combine_output_file,transformed_data, headers )

def output_mode():
    download_file(csv_input_file)
    output_csv_pdf_json(csv_input_file, csv_output_file, image_directory_path)
    save_provessed_pdf(image_directory_path)
    print("completed")
output_mode()